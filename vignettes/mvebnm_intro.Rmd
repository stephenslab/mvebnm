---
title: "Introduction to mvebnm: empirical Bayes for multivariate normal means"
author: Peter Carbonetto
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{mvebnm intro}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center")
```

Simulate data
-------------

Here we illustrate the mvebnm method in a simulated data set, in which
the data are simulated from the multivariate normal means model with 4
mixture components.

```{r load-pkgs}
library(mvebnm)
```

We simulate 4,000 data points from a mixture of multivariate
normals,
$$
w_1 N(0,S + U_1) + \cdots + w_4 N(0,S + U_4).
$$

```{r sim-data-settings}
set.seed(1)
n <- 4000
k <- 4
S <- rbind(c(0.8,0.2),
           c(0.2,1.5))
U <- list(none   = rbind(c(0,0),
                         c(0,0)),
          shared = rbind(c(1.0,0.9),
                         c(0.9,1.0)),
          only1  = rbind(c(1,0),
                         c(0,0)),
          only2  = rbind(c(0,0),
                         c(0,1)))
w <- c(0.8,0.1,0.075,0.025)
X <- simulate_mvebnm_data(n,w,U,S)
```

"Truncated-eigenvalue" EM
-------------------------

One innovation in mvebnm is that the M-step update for the prior
covariances is solved exactly by truncating the eigenvalues in a
spectral decomposition of the unconstrained maximum-likelihood
estimate. This provides greater freedom in fitting the model; here we
demonstrate this by comparing the new update, `update.U = "teem"`,
against the "Extreme Deconvolution" update, `update.U = "ed"`.

First, fit the multivariate normal means model using the original
"Extreme Deconvolution" update:

```{r fit-mvebnm-ed, results="hide"}
set.seed(1)
fit.ed <- mvebnm(X,k = k,S = S,control = list(update.U = "ed"))
```

Next, fit the model using the "truncated eigenvalue" update:

```{r fit-mvebnm-teem, results="hide"}
set.seed(1)
fit.teem <- mvebnm(X,k = k,S = S,control = list(update.U = "teem"))
```

The "truncated eigenvalue" update provides a better fit, as measured
by the likelihood:

```{r compare-loglik-1}
print(fit.ed$loglik,digits = 12)
print(fit.teem$loglik,digits = 12)
```

This is because the new update allows for much greater flexibility in
the covariance estimates:

```{r compare-prior-covs}
print(round(sapply(fit.ed$U,function (x) eigen(x)$values),digits = 6))
print(round(sapply(fit.teem$U,function (x) eigen(x)$values),digits = 6))
```

The output above shows the eigenvalues of each of the four 2 x 2
covariance matrices. The TEEM updates have shrunk some of the
eigenvalues to zero, leading to prior covariances with an effective
rank of 0, 1 or 2 --- this is the number of non-zero eigenvalues in
each column of the output above. By contrast, the prior covariances
estimated by the ED algorithm are all full rank --- all the
eigenvalues are greater than zero.

Estimate the residual covariance
--------------------------------

In cases where the residual variance-covariance matrix, $S$, is not
known, mvebnm can estimate it from the data:

```{r fit-mvebnm-estimate-resid-cov, results="hide"}
set.seed(1)
fit.teem2 <- mvebnm(X,k = k,control = list(update.S = "em",update.U = "teem"))
```

The resulting likelihood is slightly higher than the fixing the
residual covariance to the ground-truth:

```{r compare-loglik-2}
print(fit.teem$loglik,digits = 12)
print(fit.teem2$loglik,digits = 12)
```

Efficient C++ implementation
----------------------------

We provide two implementations of the mvebnm methods: an R version
with code that is easier to understand, and simpler to maintain; and a
C++ (Rcpp) version that can be much faster. Both implementations
should give the same, or nearly the same, output. Here's an example:

```{r r-vs-rcpp, results="hide"}
set.seed(1)
t1 <- system.time(
  fit1 <- mvebnm(X,k = k,control = list(update.S = "em",version = "R")))
set.seed(1)
t2 <- system.time(
  fit2 <- mvebnm(X,k = k,control = list(update.S = "em",version = "Rcpp")))
```

Indeed, both runs achieve the same likelihood (you can check for
yourself that the estimates are also the same),

```{r compare-loglik-3}
print(fit1$loglik,digits = 12)
print(fit2$loglik,digits = 12)
```

but the C++ implementation achieves the result much more quickly:

```{r compare-runtime}
print(t1)
print(t2)
```

Plot improvement in solutions over time
---------------------------------------

*Add content here.*
