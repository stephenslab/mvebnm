---
title: "Introduction to mvebnm: empirical Bayes for multivariate normal means"
author: Peter Carbonetto
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{mvebnm intro}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center")
```

*Add introductory text here.*

```{r load-pkgs}
library(mvebnm)
```

Simulate data
-------------

Here we illustrate the mvebnm method in a simulated data set, in which
the data are simulated from the multivariate normal means model with 4
mixture components. Specifically, 4,000 data points are simulated from
the mixture of multivariate normals
$$
w_1 N(0,S + U_1) + \cdots + w_4 N(0,S + U_4).
$$

```{r sim-data-settings}
set.seed(1)
n <- 4000
k <- 4
S <- rbind(c(0.8,0.2),
           c(0.2,1.5))
U <- list(none   = rbind(c(0,0),
                         c(0,0)),
          shared = rbind(c(1.0,0.9),
                         c(0.9,1.0)),
          only1  = rbind(c(1,0),
                         c(0,0)),
          only2  = rbind(c(0,0),
                         c(0,1)))
w <- c(0.8,0.1,0.075,0.025)
X <- simulate_mvebnm_data(n,w,U,S)
```

"Truncated-eigenvalue" EM algorithm
-----------------------------------

One innovation is that we solve the M-step update for the prior
covariances exactly by truncating the eigenvalues in a spectral
decomposition of the unconstrained maximum-likelihood estimate. This
provides greater freedom in fitting the model; here we compare this
update, `update.U = "teem"` against the "Extreme Deconvolution"
update, `update.U = "ed"`.

First, fit the multivariate normal means model using the original
"Extreme Deconvolution" updates.

```{r fit-mvebnm-ed, results="hide"}
set.seed(1)
fit.ed <- mvebnm(X,k = k,S = S,control = list(update.U = "ed"))
```

Fit the model a second time using the "truncated eigenvalue" updates.

```{r fit-mvebnm-teem, results="hide"}
set.seed(1)
fit.teem <- mvebnm(X,k = k,S = S,control = list(update.U = "teem"))
```

The "truncated eigenvalue" updates not only provide a better fit, as
measured by the likelihood,

```{r compare-loglik}
print(fit.ed$loglik,digits = 12)
print(fit.teem$loglik,digits = 12)
```

but also allow for much greater flexibility in the covariance
estimates:

```{r compare-prior-covs}
print(round(sapply(fit.ed$U,function (x) eigen(x)$values),digits = 6))
print(round(sapply(fit.teem$U,function (x) eigen(x)$values),digits = 6))
```

By computing the eigenvalues of each of the 2 x 2 covariance matrices,
we see that the TEEM updates have shrunk some of the eigenvalues to
zero, leading to prior covariances with an effective rank of 0, 1 and
2 --- this is the number of non-zero eigenvalues in each column of the
output above --- whereas the prior covariances estimated by the ED
algorithm are all full rank.
