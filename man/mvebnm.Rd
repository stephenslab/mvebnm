% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mvebnm.R
\name{mvebnm}
\alias{mvebnm}
\alias{mvebnm_control_default}
\title{Fit Empirical Bayes Multivariate Normal Means Model}
\usage{
mvebnm(X, k, w, U, S = diag(ncol(X)), control = list(), verbose = TRUE)

mvebnm_control_default()
}
\arguments{
\item{X}{The n x m data matrix, in which each row is an
m-dimensional observation.}

\item{k}{An integer 2 or greater specifying the number of
components in the mixture-of-normals prior. This only needs to be
provided if neither \code{U} nor \code{w} are provided.}

\item{w}{A numeric vector of length k giving initial estimates of
the prior mixture weights. All entries must be non-negative, but
need not sum to 1; the mixture weights are automatically normalized
to sum to 1. If not provided, the mixture weights are set to
uniform.}

\item{U}{Describe input argument "U" here.}

\item{S}{An m x m matrix giving the initial estimate of the
residual covariance matrix.}

\item{control}{A list of parameters controlling the behaviour of
the model fitting algorithm. See \sQuote{Details}.}
}
\value{
A list object with the following elements:

\item{item1}{Describe "item1" here.}

\item{item2}{Describe "item2" here.}
}
\description{
This function implements an empirical Bayes method for
fitting a multivariate normal means model. This method is closely
related to approaches for multivariate density deconvolution
(Sarkar \emph{et al}, 2018), so it can also be viewed as a method
for multivariate density deconvolution, In the model, each
m-dimensional observation \eqn{x} is drawn from a mixture of
multivariate normals, \eqn{x ~ w_1 N(0, T_1) + ... + w_k N(0,
T_k)}, where \eqn{k} is the number of mixture components, the
\eqn{w_j}'s are the ixture weights, and each \eqn{T_j = S + U_j} is
a covariance matrix. This is the marginal density derived from a
model in which each \eqn{x} is multivariate normal with mean
\eqn{y} and covariance \eqn{S}, and the underlying, or "latent",
signal \eqn{y} is in turn modeled by a mixture prior in which each
mixture component \eqn{j} is multivariate normal with zero mean and
covariance matrix \eqn{S_j}. The "Extreme Deconvolution" model
(Bovy \emph{et al}, 2011) is a slight generalization of this
multivariate normal means model; it allows for the mixture
components to have nonzero means, and it allows one to specify a
linear projection of the underlying signal onto the observed
signal. Therefore, this method also implements a useful special
case of Extreme Deconvolution.
}
\details{
Two variants of the EM algorithm are implemented: the EM
algorithm described by Bovy \emph{et al} (2011); and "teem" (truncated
eigenvalue expectation-maximization), in which the M-step update
for the covariance matrices U is derived from an eigenvalue
decomposition of the covariance matrices T = U + S. The latter
method provides greater freedom in the updates for U, and should
yield better fits. The choice of M-step update is specified by the
"cov.update" input argument.

Using this function requires some care; currently only minimal
argument checking is performed. See the documentation and examples
for guidance.
}
\references{
J. Bovy, D. W. Hogg and S. T. Roweis (2011). Extreme Deconvolution:
inferring complete distribution functions from noisy, heterogeneous
and incomplete observations. \emph{Annals of Applied Statistics},
\bold{5}, 1657–1677. doi:10.1214/10-AOAS439

A. Sarkar, D. Pati, A. Chakraborty, B. K. Mallick and R. J. Carroll
(2018). Bayesian semiparametric multivariate density
deconvolution. \emph{Journal of the American Statistical
Association} \bold{113}, 401–416. doi:10.1080/01621459.2016.1260467

J. Won, J. Lim, S. Kim and B. Rajaratnam
(2013). Condition-number-regularized covariance estimation.
\emph{Journal of the Royal Statistical Society, Series B} \bold{75},
427–450. doi:10.1111/j.1467-9868.2012.01049.x
}
