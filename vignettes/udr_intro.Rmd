---
title: "Introduction to Ultimate Deconvolution"
author: Peter Carbonetto
date: "`r Sys.Date()`"
output:
  html_document:
    theme: readable
vignette: >
  %\VignetteIndexEntry{Introduction to Ultimate Deconvolution}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",eval = FALSE)
```

Simulate data
-------------

Here we illustrate the Ultimate Deconvolution (UD) method in a simulated
data set, in which the data are simulated from the multivariate normal
means model with 4 mixture components.

```{r load-pkgs}
library(udr)
```

We simulate 4,000 data points from a mixture of multivariate
normals,
$$
w_1 N(0,S + U_1) + \cdots + w_4 N(0,S + U_4).
$$

```{r sim-data-settings}
set.seed(1)
n <- 4000
k <- 4
S <- rbind(c(0.8,0.2),
           c(0.2,1.5))
U <- list(none   = rbind(c(0,0),
                         c(0,0)),
          shared = rbind(c(1.0,0.9),
                         c(0.9,1.0)),
          only1  = rbind(c(1,0),
                         c(0,0)),
          only2  = rbind(c(0,0),
                         c(0,1)))
w <- c(0.8,0.1,0.075,0.025)
X <- simulate_ud_data(n,w,U,S)
```

"Truncated-eigenvalue" EM
-------------------------

One innovation in UD is that the M-step update for the prior
covariances is solved exactly by truncating the eigenvalues in a
spectral decomposition of the unconstrained maximum-likelihood
estimate. This provides greater freedom in fitting the model; here we
demonstrate this by comparing the new update, `update.U = "teem"`,
against the "Extreme Deconvolution" update, `update.U = "ed"`.

First, fit the multivariate normal means model using the original
"Extreme Deconvolution" update:

```{r fit-ud-ed, results="hide"}
set.seed(1)
fit.ed <- ud_fit(X,k = k,S = S,control = list(update.U = "ed"))
```

Next, fit the model using the "truncated eigenvalue" update:

```{r fit-ud-teem, results="hide"}
set.seed(1)
fit.teem <- ud_fit(X,k = k,S = S,control = list(update.U = "teem"))
```

The "truncated eigenvalue" update provides a better fit, as measured
by the likelihood:

```{r compare-loglik-1}
print(fit.ed$loglik,digits = 12)
print(fit.teem$loglik,digits = 12)
```

This is because the new update allows for much greater flexibility in
the covariance estimates:

```{r compare-prior-covs}
print(round(sapply(fit.ed$U,function (x) eigen(x)$values),digits = 6))
print(round(sapply(fit.teem$U,function (x) eigen(x)$values),digits = 6))
```

The output above shows the eigenvalues of each of the four 2 x 2
covariance matrices. The TEEM updates have shrunk some of the
eigenvalues to zero, leading to prior covariances with an effective
rank of 0, 1 or 2 --- this is the number of non-zero eigenvalues in
each column of the output above. By contrast, the prior covariances
estimated by the ED algorithm are all full rank --- all the
eigenvalues are greater than zero.

Estimate the residual covariance
--------------------------------

In cases where the residual variance-covariance matrix, $S$, is not
known, UD can estimate it from the data:

```{r fit-ud-estimate-resid-cov, results="hide"}
set.seed(1)
fit.teem2 <- ud_fit(X,k = k,control = list(update.S = "em",update.U = "teem"))
```

The resulting likelihood is slightly higher than the fixing the
residual covariance to the ground-truth:

```{r compare-loglik-2}
print(fit.teem$loglik,digits = 12)
print(fit.teem2$loglik,digits = 12)
```

Efficient C++ implementation
----------------------------

We provide two implementations of UD: an R version with code that is
easier to understand, and simpler to maintain; and a C++ (Rcpp)
version that can be much faster. Both implementations should give the
same, or nearly the same, output. Here's an example:

```{r r-vs-rcpp, results="hide"}
set.seed(1)
t1 <- system.time(
  fit1 <- ud_fit(X,k = k,control = list(update.S = "em",version = "R")))
set.seed(1)
t2 <- system.time(
  fit2 <- ud_fit(X,k = k,control = list(update.S = "em",version = "Rcpp")))
```

Indeed, both runs achieve the same likelihood (you can check for
yourself that the parameter estimates are also the same),

```{r compare-loglik-3}
print(fit1$loglik,digits = 12)
print(fit2$loglik,digits = 12)
```

but the C++ implementation achieves the result much more quickly:

```{r compare-runtime}
print(t1)
print(t2)
```

Update mixture weights using mix-SQP
------------------------------------

There is also the potential to use the mix-SQP algorithm to fit the
mixture weights, with estimates of the other parameters fixed. This
could be useful as a "re-fitting" step to improve the model after
first running the EM updates:

```{r fit-ud-mixsqp, results="hide"}
set.seed(1)
fit.mixsqp <- ud_fit(X,k = k,S = S,
                     control = list(maxiter = 80))
fit.mixsqp <- ud_fit(X,fit0 = fit.mixsqp,
                     control = list(update.w = "mixsqp",maxiter = 20))
```

The mix-SQP updates only achieve a slightly higher likelihood,

```{r compare-loglik-4}
print(fit.teem$loglik,digits = 12)
print(fit.mixsqp$loglik,digits = 12)
```

but, perhaps more compellingly, in this example mix-SQP assigns
(nearly) zero weight to a redundant mixture component, whereas the EM
algorithm does not:

```{r compare-mixture-weights}
print(round(fit.teem$w,digits = 3))
print(round(fit.mixsqp$w,digits = 3))
```

The benefits of the mix-SQP updates need to be explored further.

Plot improvement in solutions over time
---------------------------------------

Each EM update should improve the log-likelihood. Here we show the
improvement in the solution over time for the four variants
illustrated above: "Extreme Deconvolution" updates (dark blue);
"truncated-eigenvalue" updates with the mix-SQP re-fitting step
(dashed, yellow line), and without (solid, orange line); and
"truncated-eigenvalue" updates combined with estimating the residual
covariance (green line). 

```{r plot-progress, fig.height=5, fig.width=4}
par(mar = c(4,4,2,2))
y <- cbind(fit.ed$progress$loglik,
           fit.teem$progress$loglik,
           fit.teem2$progress$loglik,
           fit.mixsqp$progress$loglik)
y <- max(y) - y + 0.01
plot(1:100,y[,1],col = "darkblue",type = "l",log = "y",lwd = 2,
     xlab = "iteration",ylab = "dist. from best loglik",ylim = c(0.01,500))
lines(1:100,y[,2],col = "darkorange",lwd = 2)
lines(1:100,y[,3] + 0.01,col = "seagreen",lwd = 2)
lines(1:100,y[,4] + 0.01,col = "gold",lwd = 2,lty = "dashed")
```

The combination of the "truncated-eigenvalue" updates and estimation
of the residual covariance provides the best fit in this case.
